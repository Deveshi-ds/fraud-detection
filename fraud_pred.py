# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vm0uSAImDsrvq3wELKKsVdSIo-zbMv8-
"""

import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from scipy import stats
from statsmodels.stats.outliers_influence import variance_inflation_factor
import matplotlib.pyplot as plt
import seaborn as sns

# Loading dataset
df = pd.read_csv('/content/Fraud.csv')
print(df.head(7))

print()
# 1. Handling Missing Values
# Check for missing values
missing_values = df.isnull().sum()
print("Missing values before treatment:\n", missing_values)
print()
# Impute missing values with the median for numerical columns
imputer = SimpleImputer(strategy='median')

for column in ['oldbalanceOrg', 'oldbalanceDest', 'newbalanceOrig', 'newbalanceDest']:
    if column in df.columns:
        df[column] = imputer.fit_transform(df[[column]])

missing_values = df.isnull().sum()
print("Missing values after treatment:\n", missing_values)

print()

# Remove rows with missing values
df=df.dropna()

# Print the shape of the original and cleaned DataFrames
print("cleaned dataset shape: {df.shape}")


missing_values = df.isnull().sum()
print("Missing values after treatment:\n", missing_values)

from statsmodels.tools.tools import add_constant
numerical_cols = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']

# Add constant for VIF calculation
X = df[numerical_cols]
X = add_constant(X)  # Adds a constant term to the predictor

# Calculate VIF for each feature
def calculate_vif(X):
    vif_data = pd.DataFrame()
    vif_data["feature"] = X.columns
    vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    return vif_data

vif = calculate_vif(X)
print("\nVariance Inflation Factor (VIF):")
print(vif)

# Plot VIF values
plt.figure(figsize=(10, 6))
plt.barh(vif['feature'], vif['VIF'], color='skyblue')
plt.xlabel('VIF')
plt.title('Variance Inflation Factor (VIF) for each feature')
plt.show()

# Define numerical columns
numerical_cols = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']

# 1. Handle Outliers using IQR
def detect_and_treat_outliers(df, columns):
    cleaned_df = df.copy()
    for column in columns:
        Q1 = cleaned_df[column].quantile(0.25)
        Q3 = cleaned_df[column].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Detect outliers
        outliers = (cleaned_df[column] < lower_bound) | (cleaned_df[column] > upper_bound)
        print(f"\nNumber of outliers detected in {column}: {outliers.sum()}")

        # Remove outliers
        df = cleaned_df[~outliers]

    return df

# Treat outliers
df = detect_and_treat_outliers(df, numerical_cols)

# Visualize Outliers using Box Plots
plt.figure(figsize=(12, 8))
for i, column in enumerate(numerical_cols, 1):
    plt.subplot(2, 3, i)
    sns.boxplot(df[column], color='lightblue')
    plt.title(f'Box Plot for {column}')
plt.tight_layout()
plt.show()

# 2. Address Multicollinearity using VIF
def calculate_vif(X):
    vif_data = pd.DataFrame()
    vif_data["feature"] = X.columns
    vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    return vif_data

def remove_high_vif_features(df, threshold=10):
    X = add_constant(df)
    vif = calculate_vif(X)
    while vif['VIF'].max() > threshold:
        remove = vif.sort_values('VIF', ascending=False).iloc[0]['feature']
        df = df.drop(columns=[remove])
        X = add_constant(df)
        vif = calculate_vif(X)
    return df

# Prepare data for VIF calculation
df_numerical = df[numerical_cols]

# Remove features with high VIF
df_numerical_reduced = remove_high_vif_features(df_numerical)

# Calculate VIF for the reduced DataFrame
vif_reduced = calculate_vif(add_constant(df_numerical_reduced))
print("\nReduced Variance Inflation Factor (VIF):")
print(vif_reduced)

# Plot VIF values for reduced DataFrame
plt.figure(figsize=(10, 6))
plt.barh(vif_reduced['feature'], vif_reduced['VIF'], color='skyblue')
plt.xlabel('VIF')
plt.title('Variance Inflation Factor (VIF) for Each Feature (After Removal)')
plt.show()

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, roc_auc_score


# Prepare the features and target variable
X = df[numerical_cols]
y = df['isFraud']

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize and train the Random Forest model
model = RandomForestClassifier(n_estimators=900, random_state=42)
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)
y_pred_prob = model.predict_proba(X_test)[:, 1]  # Get the probabilities for the positive class

# Evaluate the model
print("Classification Report:")
print(classification_report(y_test, y_pred))

print("ROC AUC Score:")
print(roc_auc_score(y_test, y_pred_prob))

# Feature importance plot
importances = model.feature_importances_
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10, 6))
sns.barplot(x=importances[indices], y=np.array(numerical_cols)[indices], palette='viridis')
plt.title('Feature Importance')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

from sklearn.model_selection import cross_val_score
import numpy as np

# Initialize the Random Forest model
model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')

# Perform cross-validation
cv_scores = cross_val_score(model, X, y, cv=5, scoring='f1')

print("Cross-Validation Scores (F1-Score):", cv_scores)
print("Mean F1-Score:", np.mean(cv_scores))
print("Standard Deviation of F1-Scores:", np.std(cv_scores))

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Initialize the Linear Regression model
linear_model = LinearRegression()

# Perform cross-validation
cv_scores = cross_val_score(linear_model, X, y, cv=5, scoring='r2')

print("Cross-Validation Scores (R2):", cv_scores)
print("Mean R2 Score:", np.mean(cv_scores))
print("Standard Deviation of R2 Scores:", np.std(cv_scores))

# Fit the model and evaluate on the test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
linear_model.fit(X_train, y_train)
y_pred = linear_model.predict(X_test)

# Print the evaluation metrics
print("\nEvaluation on Test Set:")
print("Mean Squared Error:", mean_squared_error(y_test, y_pred))
print("R2 Score:", r2_score(y_test, y_pred))

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder ,  LabelEncoder
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
import numpy as np


# Define features and target
X = df.drop(['isFraud'], axis=1)  # Drop the target column from features
y = df['isFraud']

# Convert categorical variables to numerical using OneHotEncoding

# Encoding all columns with string values
le = LabelEncoder()
X_encoded = X.apply(lambda col: le.fit_transform(col.astype(str)) if col.dtype == 'object' else col)

# Standardizing the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_encoded)

# Creating a Random Forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# Apply PCA
pca = PCA(n_components=None)  # Start with all components
X_pca = pca.fit_transform(X_scaled)

# Plot explained variance ratio
plt.figure(figsize=(10, 6))
plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('Explained Variance by Components')
plt.grid(True)
plt.show()

# Determine the number of components to keep (e.g., 95% variance)
explained_variance = np.cumsum(pca.explained_variance_ratio_)
n_components = np.argmax(explained_variance >= 0.95) + 1
print(f"Number of components to retain 95% variance: {n_components}")

# Apply PCA with the chosen number of components
pca = PCA(n_components=n_components)
X_pca = pca.fit_transform(X_scaled)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=42)

# Train a model (e.g., Random Forest)
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

# Predict and evaluate
y_pred = model.predict(X_test)
print("\nClassification Report with PCA:")
print(classification_report(y_test, y_pred))

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.pipeline import make_pipeline


# Train a model with class weights
model = RandomForestClassifier(random_state=42, class_weight='balanced')
model.fit(X_train, y_train)

# Predict and evaluate
y_pred = model.predict(X_test)

print("\nClassification Report with Class Weights:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# ROC-AUC Score
roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])
print("\nROC-AUC Score:", roc_auc)

# Creating a pipeline with PCA and Random Forest
pipeline = make_pipeline(PCA(n_components=0.95), rf_classifier)

# Performing cross-validation
cv_scores = cross_val_score(pipeline, X_scaled, y, cv=5)  # 5-fold cross-validation

# Printing the cross-validation scores
print("Cross-Validation Scores:", cv_scores)
print("Mean Accuracy:", cv_scores.mean())

from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.pipeline import Pipeline
from imblearn.pipeline import Pipeline as imPipeline
import numpy as np

# Encoding all columns with string values
le = LabelEncoder()
X_encoded = X.apply(lambda col: le.fit_transform(col.astype(str)) if col.dtype == 'object' else col)

# Standardizing the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_encoded)

# Splitting the dataset
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

# Applying SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# Initialize Random Forest model
rf_classifier = RandomForestClassifier(random_state=42, class_weight='balanced')

# Train the Random Forest model with the resampled data
rf_classifier.fit(X_resampled, y_resampled)

# Predict and evaluate on the test set
y_pred = rf_classifier.predict(X_test)

print("\nClassification Report with SMOTE:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
print("\nConfusion Matrix with SMOTE:")
print(confusion_matrix(y_test, y_pred))

# ROC-AUC Score
roc_auc = roc_auc_score(y_test, rf_classifier.predict_proba(X_test)[:, 1])
print("\nROC-AUC Score with SMOTE:", roc_auc)

# Creating a pipeline with SMOTE and Random Forest
pipeline = imPipeline([
    ('smote', SMOTE(random_state=42)),
    ('rf', RandomForestClassifier(random_state=42, class_weight='balanced'))
])

# Performing cross-validation
cv_scores = cross_val_score(pipeline, X_scaled, y, cv=5, scoring='f1')  # 5-fold cross-validation using F1 score

# Printing the cross-validation scores
print("Cross-Validation Scores (F1-Score):", cv_scores)
print("Mean F1-Score:", np.mean(cv_scores))
print("Standard Deviation of F1-Scores:", np.std(cv_scores))

import xgboost as xgb
from xgboost import XGBClassifier

# Initialize the XGBoost model
xgb_model = XGBClassifier(random_state=42)

# Train the XGBoost model
xgb_model.fit(X_resampled, y_resampled)

# Predict and evaluate
y_pred_xgb = xgb_model.predict(X_test)
print("\nClassification Report with XGBoost:")
print(classification_report(y_test, y_pred_xgb))

pip install optuna

import optuna
from xgboost import XGBClassifier
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split

# Define the objective function for Optuna
def objective(trial):
    param = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 300),
        'learning_rate': trial.suggest_uniform('learning_rate', 0.01, 0.2),
        'max_depth': trial.suggest_int('max_depth', 3, 15),
        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),
        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),
        'gamma': trial.suggest_uniform('gamma', 0, 0.5)
    }

    model = XGBClassifier(**param, random_state=42)
    model.fit(X_resampled, y_resampled)
    y_pred = model.predict(X_test)
    return f1_score(y_test, y_pred)

# Initialize Optuna study
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)

# Best parameters and score
print("Best Parameters:", study.best_params)
print("Best F1-Score:", study.best_value)

# Use the best model
best_xgb_optuna = XGBClassifier(**study.best_params, random_state=42)
best_xgb_optuna.fit(X_resampled, y_resampled)

import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

# Predict and evaluate using the best model
y_pred = best_xgb_optuna.predict(X_test)

print("\nClassification Report with Optuna-tuned XGBoost:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# ROC-AUC Score
roc_auc = roc_auc_score(y_test, best_xgb_optuna.predict_proba(X_test)[:, 1])
print("\nROC-AUC Score:", roc_auc)

# Plot feature importances
importances = best_xgb_optuna.feature_importances_
feature_names = X.columns
sorted_idx = importances.argsort()

plt.figure(figsize=(10, 8))
plt.barh(range(len(sorted_idx)), importances[sorted_idx], align='center')
plt.yticks(range(len(sorted_idx)), feature_names[sorted_idx])
plt.xlabel('Feature Importance')
plt.title('Feature Importance from Optuna-tuned XGBoost')
plt.show()

from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, best_xgb_optuna.predict_proba(X_test)[:, 1])
plt.figure(figsize=(10, 5))
plt.plot(fpr, tpr, marker='o', label='ROC curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend()
plt.show()

# Precision-Recall Curve
precision, recall, _ = precision_recall_curve(y_test, best_xgb_optuna.predict_proba(X_test)[:, 1])
plt.figure(figsize=(10, 5))
plt.plot(recall, precision, marker='o', label='Precision-Recall curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.show()